{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to MkDocs\n\n\nFor full documentation visit \nmkdocs.org\n.\n\n\nCommands\n\n\n\n\nmkdocs new [dir-name]\n - Create a new project.\n\n\nmkdocs serve\n - Start the live-reloading docs server.\n\n\nmkdocs build\n - Build the documentation site.\n\n\nmkdocs help\n - Print this help message.\n\n\n\n\nProject layout\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-mkdocs", 
            "text": "For full documentation visit  mkdocs.org .", 
            "title": "Welcome to MkDocs"
        }, 
        {
            "location": "/#commands", 
            "text": "mkdocs new [dir-name]  - Create a new project.  mkdocs serve  - Start the live-reloading docs server.  mkdocs build  - Build the documentation site.  mkdocs help  - Print this help message.", 
            "title": "Commands"
        }, 
        {
            "location": "/#project-layout", 
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.", 
            "title": "Project layout"
        }, 
        {
            "location": "/README/", 
            "text": "Crawtext\n\n\nUn projet pour la plateforme Cortext\n\n\nCrawtext est un \ncrawler\n \nou un \nrobot d'indexation de texte\n \nqui permet la constitution de gros corpus web textuels\nissus de page web\nautour d'une expression de recherche donn\u00e9e \nde mani\u00e8re r\u00e9currente selon la fr\u00e9quence souhait\u00e9e.\n\n\nInitialement pr\u00e9vue pour \u00eatre int\u00e9gr\u00e9e \u00e0 la plateforme Cortext Manager comme un outil de constitution de datasets issu du web. \nElle fonctionne pour le moment en mode console et de mani\u00e8re ind\u00e9pendante.\n\n\nVous trouverez donc ici la documentation compl\u00e8te sur Crawtext ainsi que des cas d'utilisation\n\n \nIntroduction\n\n  * Qu'est ce que Crawtext?\n  * A quoi \u00e7a sert?\n  * Comment \u00e7a marche?\n  * Les strat\u00e9gies de crawl \n \nInstallation\n\n\n \nConfiguration\n\n\n \nTutoriel", 
            "title": "Summary"
        }, 
        {
            "location": "/README/#crawtext", 
            "text": "Un projet pour la plateforme Cortext  Crawtext est un  crawler  \nou un  robot d'indexation de texte  \nqui permet la constitution de gros corpus web textuels\nissus de page web\nautour d'une expression de recherche donn\u00e9e \nde mani\u00e8re r\u00e9currente selon la fr\u00e9quence souhait\u00e9e.  Initialement pr\u00e9vue pour \u00eatre int\u00e9gr\u00e9e \u00e0 la plateforme Cortext Manager comme un outil de constitution de datasets issu du web. \nElle fonctionne pour le moment en mode console et de mani\u00e8re ind\u00e9pendante.  Vous trouverez donc ici la documentation compl\u00e8te sur Crawtext ainsi que des cas d'utilisation   Introduction \n  * Qu'est ce que Crawtext?\n  * A quoi \u00e7a sert?\n  * Comment \u00e7a marche?\n  * Les strat\u00e9gies de crawl    Installation    Configuration    Tutoriel", 
            "title": "Crawtext"
        }, 
        {
            "location": "/introduction/", 
            "text": "Qu'est ce que Crawtext?\n\n\nCrawtext est un \ncrawler\n \nou un \nrobot d'indexation de texte\n \nou encore appel\u00e9 un \nmoissonneur\n en fran\u00e7ais\nC'est un robot qui permet la constitution de gros corpus web textuels\nissus de page web\nautour d'une expression de recherche donn\u00e9e \nde mani\u00e8re r\u00e9currente selon la fr\u00e9quence souhait\u00e9e.\n\n\nA quoi ca sert?\n\n\nCrawtext est un moissonneur d'informations issues des pages web. Il permet \u00e0 un utilisateur de collecter les informations contenues dans les pages web autour d'un sujet et ce sur une fr\u00e9quence r\u00e9guli\u00e8re. Il permet ainsi de collecter ainsi des informations du web (acteurs, contenus des d\u00e9bats, liens entre acteur) de mani\u00e8re historicis\u00e9e (dans son \u00e9volution) et centr\u00e9e autour d'une question.\n\n\nComment ca marche?\n\n\nCrawtext est un moissonneur du web qui suit quelques principes simples. \n\n\nIci\n pour ceux qui le souhaite, un petit rappel utile pour comprendre le fonctionnement de ce robot sur ce qu'est le web, Internet et un site web. \n\n\nA partir d'une page web de d\u00e9part appel\u00e9e \nseed\n, \nle robot crawler (qu'on compare \u00e0 une araign\u00e9e) visite la page et recup\u00e8re dans la page html tous les liens disponibles qui prennent la forme de :\n\na href=\"lien\nancre\n/a\n\n\nIl collecte et stocke les liens puis visite une \u00e0 une les pages, r\u00e9cup\u00e8re les liens, charge la page puis v\u00e9rifie que l'expression de recherche est bien dans la page, si l'expression est trouv\u00e9e il recommence le processus jusqu'\u00e0 ce que plus une page ne soit  pertinente.\nEn pseudo code simplifi\u00e9 cela donnerait\n\n\nqueue = liste des urls de d\u00e9part\ntant que la queue de traitement n'est pas vide:\n  pour chaque url:\n    crawler le contenu de l'url\n    si la requ\u00eate est pr\u00e9sente dans le contenu\n      * on stocke l'information \n      * on ajoute les nouvelles urls cit\u00e9es dans la queue de traitement\n      * on supprime l'url de la queue de traitement\n\n\n\n\n\n\nStrat\u00e9gies de crawl\n\n\nTypes de crawl\n\n\nCrawtext est un crawler web qui poss\u00e8de plusieurs types de comportement. \nqui correspondent \u00e0 plusieurs strat\u00e9gies de crawl param\u00e9trables par l'utilisateur:\n\n crawl d'un sujet/th\u00e9matique exprim\u00e9e \u00e0 travers une expression de recherche\n\n crawl d'un site web complet\n* crawl mixte sur un ou plusieurs sites web sources d'un sujet ou th\u00e9matique \nexprim\u00e9e \u00e0 travers une expression de recherche\n\n\nLe crawl n\u00e9cessite un point de d\u00e9part pour d\u00e9marrer son parcours appel\u00e9s \nseeds\n\nPlusieurs m\u00e9thodes sont propos\u00e9es qui peuvent \u00eatre mix\u00e9s en ajoutant:\n\n une url simple\n\n un fichier contenant une url par ligne\n* une cl\u00e9 d'API au moteur de recherche \nBING\n: \n    * on peut l'obtenir en s'inscrivant \nici\n\n    * une expression de recherche est alors indispensable au fonctionnement du crawl parce que les r\u00e9sultats de recherche constitue le point de d\u00e9part du crawl\n\n\nFiltres\n\n\nPlusieurs filtres additionnels sont propos\u00e9s:\n\n\n\n\n\n\nun filtre de langue: le crawler selectionnera uniquement les textes qui correspondent \u00e0 la langue selectionn\u00e9e au format \nISO 639-1\n\n\n\n\n\n\nun filtre de profondeur de crawl: le crawler arretera la recherche quand le nombre d'\u00e9tape sera atteint.\n      Ce filtre permet de r\u00e9duire \n      les temps de traitement souvent tr\u00e8s long\n      et de controler les \u00e9largissements successifs \n      du p\u00e9rim\u00eatre de recherche\n\n\n\n\n\n\nun filtre de recherche: \nle crawler selectionnera uniquement les textes qui correspondent \u00e0 cette expression de recherche.\n\n\nCe filtre est indispensable dans le cas d'un crawl autour \n  d'un sujet ou d'une th\u00e9matique\n\n\n\n\n\n\nPour plus d'information sur les filtres: voir \nConfiguration\n\n\nFr\u00e9quence\n\n\nOn peut programmer la \nr\u00e9currence\n du crawl en sp\u00e9cifiant une fr\u00e9quence:\n * journali\u00e8re\n * hebdomadaire \n * ou mensuelle\n\n\nPour plus d'information sur les filtres: voir \nConfiguration\n\n\nLimitations de Crawtext\n\n\nInterface web\n\n\nLe crawler ne poss\u00e8de pas d'interface de configuration web pour le moment\n\n\nType de donn\u00e9es collect\u00e9es\n\n\nLe crawler Crawtext ne r\u00e9cup\u00e8re pas tous les types d'information disponible sur le web. \n\n\nSont exclus (pour le moment): \n\n pdf\n\n fichiers\n\n videos\n\n sons \n\n images\n\n flash \n\n\nEn revanche, crawtext stocke \u00e0 la fois:\n\n la \npage html brute\n \n\n le \ntexte\n de la page nettoy\u00e9e\n* des informations contextuelles suppl\u00e9mentaires\n\n\nBlockage des pubs et r\u00e9seaux sociaux\n\n\nLe crawler Crawtext bloque les pages commerciales (pubs, questionnaires, pop-ups) et les r\u00e9seaux sociaux en utilisant un fichier AdBlocks qui n'est \u00e0 ce stade pas configurable et n'est pas mis \u00e0 jour automatiquement pour le moment.\nLes sites webs dont les contenus sont charg\u00e9s dynamiquement ne sont pas non plus support\u00e9s.\n\n\nTemps de traitement et capacit\u00e9 de stockage\n\n\nEn fonction du nombre d'url de d\u00e9part, de la finesse de la requete, le crawler peut mettre de quleques heures \u00e0 plusieurs jours \u00e0 compl\u00e9ter ses t\u00e2ches et les donn\u00e9es collect\u00e9es peuvent prendre \u00e9norm\u00e9ment de place.\nIl faut donc etre bien attentif \u00e0 calibrer son crawl avant de le lancer et de tester pour le calibrer.\n\n\n\n\nPour plus d'information sur l'\u00e9tat de l'art et les limitations: voir \nDevelopper Guide\n\net les choses \u00e0 faire \nTODO", 
            "title": "Introduction"
        }, 
        {
            "location": "/introduction/#quest-ce-que-crawtext", 
            "text": "Crawtext est un  crawler  \nou un  robot d'indexation de texte  \nou encore appel\u00e9 un  moissonneur  en fran\u00e7ais\nC'est un robot qui permet la constitution de gros corpus web textuels\nissus de page web\nautour d'une expression de recherche donn\u00e9e \nde mani\u00e8re r\u00e9currente selon la fr\u00e9quence souhait\u00e9e.", 
            "title": "Qu'est ce que Crawtext?"
        }, 
        {
            "location": "/introduction/#a-quoi-ca-sert", 
            "text": "Crawtext est un moissonneur d'informations issues des pages web. Il permet \u00e0 un utilisateur de collecter les informations contenues dans les pages web autour d'un sujet et ce sur une fr\u00e9quence r\u00e9guli\u00e8re. Il permet ainsi de collecter ainsi des informations du web (acteurs, contenus des d\u00e9bats, liens entre acteur) de mani\u00e8re historicis\u00e9e (dans son \u00e9volution) et centr\u00e9e autour d'une question.", 
            "title": "A quoi ca sert?"
        }, 
        {
            "location": "/introduction/#comment-ca-marche", 
            "text": "Crawtext est un moissonneur du web qui suit quelques principes simples.   Ici  pour ceux qui le souhaite, un petit rappel utile pour comprendre le fonctionnement de ce robot sur ce qu'est le web, Internet et un site web.   A partir d'une page web de d\u00e9part appel\u00e9e  seed , \nle robot crawler (qu'on compare \u00e0 une araign\u00e9e) visite la page et recup\u00e8re dans la page html tous les liens disponibles qui prennent la forme de : a href=\"lien ancre /a  Il collecte et stocke les liens puis visite une \u00e0 une les pages, r\u00e9cup\u00e8re les liens, charge la page puis v\u00e9rifie que l'expression de recherche est bien dans la page, si l'expression est trouv\u00e9e il recommence le processus jusqu'\u00e0 ce que plus une page ne soit  pertinente.\nEn pseudo code simplifi\u00e9 cela donnerait  queue = liste des urls de d\u00e9part\ntant que la queue de traitement n'est pas vide:\n  pour chaque url:\n    crawler le contenu de l'url\n    si la requ\u00eate est pr\u00e9sente dans le contenu\n      * on stocke l'information \n      * on ajoute les nouvelles urls cit\u00e9es dans la queue de traitement\n      * on supprime l'url de la queue de traitement", 
            "title": "Comment ca marche?"
        }, 
        {
            "location": "/introduction/#strategies-de-crawl", 
            "text": "", 
            "title": "Strat\u00e9gies de crawl"
        }, 
        {
            "location": "/introduction/#types-de-crawl", 
            "text": "Crawtext est un crawler web qui poss\u00e8de plusieurs types de comportement. \nqui correspondent \u00e0 plusieurs strat\u00e9gies de crawl param\u00e9trables par l'utilisateur:  crawl d'un sujet/th\u00e9matique exprim\u00e9e \u00e0 travers une expression de recherche  crawl d'un site web complet\n* crawl mixte sur un ou plusieurs sites web sources d'un sujet ou th\u00e9matique \nexprim\u00e9e \u00e0 travers une expression de recherche  Le crawl n\u00e9cessite un point de d\u00e9part pour d\u00e9marrer son parcours appel\u00e9s  seeds \nPlusieurs m\u00e9thodes sont propos\u00e9es qui peuvent \u00eatre mix\u00e9s en ajoutant:  une url simple  un fichier contenant une url par ligne\n* une cl\u00e9 d'API au moteur de recherche  BING : \n    * on peut l'obtenir en s'inscrivant  ici \n    * une expression de recherche est alors indispensable au fonctionnement du crawl parce que les r\u00e9sultats de recherche constitue le point de d\u00e9part du crawl", 
            "title": "Types de crawl"
        }, 
        {
            "location": "/introduction/#filtres", 
            "text": "Plusieurs filtres additionnels sont propos\u00e9s:    un filtre de langue: le crawler selectionnera uniquement les textes qui correspondent \u00e0 la langue selectionn\u00e9e au format  ISO 639-1    un filtre de profondeur de crawl: le crawler arretera la recherche quand le nombre d'\u00e9tape sera atteint.\n      Ce filtre permet de r\u00e9duire \n      les temps de traitement souvent tr\u00e8s long\n      et de controler les \u00e9largissements successifs \n      du p\u00e9rim\u00eatre de recherche    un filtre de recherche: \nle crawler selectionnera uniquement les textes qui correspondent \u00e0 cette expression de recherche.  Ce filtre est indispensable dans le cas d'un crawl autour \n  d'un sujet ou d'une th\u00e9matique    Pour plus d'information sur les filtres: voir  Configuration", 
            "title": "Filtres"
        }, 
        {
            "location": "/introduction/#frequence", 
            "text": "On peut programmer la  r\u00e9currence  du crawl en sp\u00e9cifiant une fr\u00e9quence:\n * journali\u00e8re\n * hebdomadaire \n * ou mensuelle  Pour plus d'information sur les filtres: voir  Configuration", 
            "title": "Fr\u00e9quence"
        }, 
        {
            "location": "/introduction/#limitations-de-crawtext", 
            "text": "", 
            "title": "Limitations de Crawtext"
        }, 
        {
            "location": "/introduction/#interface-web", 
            "text": "Le crawler ne poss\u00e8de pas d'interface de configuration web pour le moment", 
            "title": "Interface web"
        }, 
        {
            "location": "/introduction/#type-de-donnees-collectees", 
            "text": "Le crawler Crawtext ne r\u00e9cup\u00e8re pas tous les types d'information disponible sur le web.   Sont exclus (pour le moment):   pdf  fichiers  videos  sons   images  flash   En revanche, crawtext stocke \u00e0 la fois:  la  page html brute    le  texte  de la page nettoy\u00e9e\n* des informations contextuelles suppl\u00e9mentaires", 
            "title": "Type de donn\u00e9es collect\u00e9es"
        }, 
        {
            "location": "/introduction/#blockage-des-pubs-et-reseaux-sociaux", 
            "text": "Le crawler Crawtext bloque les pages commerciales (pubs, questionnaires, pop-ups) et les r\u00e9seaux sociaux en utilisant un fichier AdBlocks qui n'est \u00e0 ce stade pas configurable et n'est pas mis \u00e0 jour automatiquement pour le moment.\nLes sites webs dont les contenus sont charg\u00e9s dynamiquement ne sont pas non plus support\u00e9s.", 
            "title": "Blockage des pubs et r\u00e9seaux sociaux"
        }, 
        {
            "location": "/introduction/#temps-de-traitement-et-capacite-de-stockage", 
            "text": "En fonction du nombre d'url de d\u00e9part, de la finesse de la requete, le crawler peut mettre de quleques heures \u00e0 plusieurs jours \u00e0 compl\u00e9ter ses t\u00e2ches et les donn\u00e9es collect\u00e9es peuvent prendre \u00e9norm\u00e9ment de place.\nIl faut donc etre bien attentif \u00e0 calibrer son crawl avant de le lancer et de tester pour le calibrer.   Pour plus d'information sur l'\u00e9tat de l'art et les limitations: voir  Developper Guide \net les choses \u00e0 faire  TODO", 
            "title": "Temps de traitement et capacit\u00e9 de stockage"
        }, 
        {
            "location": "/installation/", 
            "text": "Installation\n\n\n\n\nApercu des briques logicielles\n\n\nInstaller MongoDB\n\n\nMise en place d'un virtualenv\n\n\nInstallation du parser LXML\n\n\nInstallation des d\u00e9pendances du projet\n\n\n\n\nApercu des briques logicielles\n\n\nCrawtext est \u00e9crit en Python 2.7 avec une base de donn\u00e9es Mongo 3.2\n\n\n4 \u00e9tapes d'installation:\n\n\n\n\n\n\ninstallation de la base de donn\u00e9es en Backend MongoDB \n\n\n\n\n\n\ninstallation du parser lxml:\n\n\nOn peut rencontrer certains probl\u00e8mes \u00e0 l'installation du package python ```lxml```\nque nous contournons ici\n\n\n\n\n\n\n\ncr\u00e9ation d'un environnement virtuel et installation des packages suppl\u00e9mentaire\n\n\nIl est recommand\u00e9 d'isoler l'installation de Crawtext dans un ```virtualenv```\net profiter du syst\u00e8me d'installation simplifi\u00e9e avec ```pip```\n\n\n\n\n\n\n\ncloner le repository de crawtext\n\n\nOu simplement le t\u00e9l\u00e9charger en zip\n\n\n\n\n\n\n\nFor implementation choice and the necessary what I learnt \ncf. Developper Guide\n\n\nNext steps installation are in \nEnglish\n. \nD\u00e9sol\u00e9 les gars, j'ai tout commenc\u00e9 en anglais\n\n\nInstall MongoDB\n\n\nMongo has to be install first and outside the environnement \n\n\n\n\n\n\nOn LINUX (Debian based distribution):\nPackages are compatibles with:\n\n\n\n\n\n\nDebian 7 Wheezy (and older)\n\n\n\n\n\n\nUbuntu 12.04 LTS and 14.04 LTS (and older)\n\n\n\n\n\n\n\n\n\n\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv EA312927\necho \ndeb http://repo.mongodb.org/apt/debian wheezy/mongodb-org/3.2 main\n | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.list\nsudo apt-get update\nsudo apt-get install -y mongodb-org=3.2.1 mongodb-org-server=3.2.1 mongodb-org-shell=3.2.1 mongodb-org-mongos=3.2.1 mongodb-org-tools=3.2.1\n\n\n\n\n\n\nOn MAC OS/X: \n(from LionX to newest)\n\n\n\n\nbrew update\nbrew install mongodb --with-openssl\n\n\n\n\n\n\n\n\nOn Windows:\n\n\nrefer to \nofficial MongoDB installation procedure\n\n\n\n\n\n\nLet's verify now that mongo is running properly\n\n\n$ mongo\nMongoDB shell version: 3.2.0\nconnecting to: test\n\n\n\n\n\n\nType Ctrl+C to quit\n\n\nInstall LXML\n\n\nInstall LXML may cause some troubelshooting: to avoid it install \nadditionnal packages outside the environnement\n\n\n\n\nOn Debian\n\n\n\n\nsudo apt-get install libxml2-dev libxslt-dev python-dev\n\n\n\n\n\n\nOn MAC\n\n\n\n\nbrew install libxml2\nbrew install libxslt\nbrew link libxml2 --force\nbrew link libxslt --force\n\n\n\n\n\n\nOn Windows\n\n\n\n\nSelect the source file that corresponds to you architecture (32 or 64 bits)\nopen an run it\n\nlxml ditributions\n\n\nCreate a virtualenv\n\n\nVerify that virtualenv is installed\n\n\n$ virtualenv --version\n\n\n\n\nIf you got a \u201cCommand not found\u201d when you tried to use virtualenv, try:\n\n\n$ sudo pip install virtualenv\n\n\n\n\nor\n\n\nsudo apt-get install python-virtualenv # for a Debian-based system\n\n\n\n\nCreate a cortext-box and activate the virtual-env\n\n\n$ virtualenv cortext-box\n$ cd cortext-box\n$ source bin/activate\n\n\n\n\n(source bin/deactivate to exit)\n\n\nClone the repository\n\n\n$ git clone https://github.com/cortext/crawtext\n$ cd crawtext\n\n\n\n\nor simply download it\nand install additional packages using the requirements file\n\n\n$ pip install -r requirements.pip\n\n\n\n\n\nAnd that's all for now folks!\n\n\nLet's see now :\n\n\n\n\n\n\nhow to configure the crawtext \nenvironnement\n\n\n\n\n\n\nand make \nour first project", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#installation", 
            "text": "Apercu des briques logicielles  Installer MongoDB  Mise en place d'un virtualenv  Installation du parser LXML  Installation des d\u00e9pendances du projet", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#apercu-des-briques-logicielles", 
            "text": "Crawtext est \u00e9crit en Python 2.7 avec une base de donn\u00e9es Mongo 3.2  4 \u00e9tapes d'installation:    installation de la base de donn\u00e9es en Backend MongoDB     installation du parser lxml:  On peut rencontrer certains probl\u00e8mes \u00e0 l'installation du package python ```lxml```\nque nous contournons ici    cr\u00e9ation d'un environnement virtuel et installation des packages suppl\u00e9mentaire  Il est recommand\u00e9 d'isoler l'installation de Crawtext dans un ```virtualenv```\net profiter du syst\u00e8me d'installation simplifi\u00e9e avec ```pip```    cloner le repository de crawtext  Ou simplement le t\u00e9l\u00e9charger en zip    For implementation choice and the necessary what I learnt  cf. Developper Guide  Next steps installation are in  English . \nD\u00e9sol\u00e9 les gars, j'ai tout commenc\u00e9 en anglais", 
            "title": "Apercu des briques logicielles"
        }, 
        {
            "location": "/installation/#install-mongodb", 
            "text": "Mongo has to be install first and outside the environnement     On LINUX (Debian based distribution):\nPackages are compatibles with:    Debian 7 Wheezy (and older)    Ubuntu 12.04 LTS and 14.04 LTS (and older)      sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv EA312927\necho  deb http://repo.mongodb.org/apt/debian wheezy/mongodb-org/3.2 main  | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.list\nsudo apt-get update\nsudo apt-get install -y mongodb-org=3.2.1 mongodb-org-server=3.2.1 mongodb-org-shell=3.2.1 mongodb-org-mongos=3.2.1 mongodb-org-tools=3.2.1   On MAC OS/X: \n(from LionX to newest)   brew update\nbrew install mongodb --with-openssl    On Windows:  refer to  official MongoDB installation procedure    Let's verify now that mongo is running properly  $ mongo\nMongoDB shell version: 3.2.0\nconnecting to: test   Type Ctrl+C to quit", 
            "title": "Install MongoDB"
        }, 
        {
            "location": "/installation/#install-lxml", 
            "text": "Install LXML may cause some troubelshooting: to avoid it install \nadditionnal packages outside the environnement   On Debian   sudo apt-get install libxml2-dev libxslt-dev python-dev   On MAC   brew install libxml2\nbrew install libxslt\nbrew link libxml2 --force\nbrew link libxslt --force   On Windows   Select the source file that corresponds to you architecture (32 or 64 bits)\nopen an run it lxml ditributions", 
            "title": "Install LXML"
        }, 
        {
            "location": "/installation/#create-a-virtualenv", 
            "text": "Verify that virtualenv is installed  $ virtualenv --version  If you got a \u201cCommand not found\u201d when you tried to use virtualenv, try:  $ sudo pip install virtualenv  or  sudo apt-get install python-virtualenv # for a Debian-based system  Create a cortext-box and activate the virtual-env  $ virtualenv cortext-box\n$ cd cortext-box\n$ source bin/activate  (source bin/deactivate to exit)", 
            "title": "Create a virtualenv"
        }, 
        {
            "location": "/installation/#clone-the-repository", 
            "text": "$ git clone https://github.com/cortext/crawtext\n$ cd crawtext  or simply download it\nand install additional packages using the requirements file  $ pip install -r requirements.pip  And that's all for now folks!  Let's see now :    how to configure the crawtext  environnement    and make  our first project", 
            "title": "Clone the repository"
        }, 
        {
            "location": "/configuration/", 
            "text": "Configuration\n\n\nCrawtext se configure \u00e0 deux niveaux:\n\n Environnement\n\n Projet\n\n\nEnvironnement par d\u00e9faut\n\n\nCrawtext propose un fichier de configuration par d\u00e9faut: \nconfig/settings.json\n\n\nCe fichier d\u00e9finit l'environnement de fonctionnement du crawler:\n\n l'utilisateur du crawler: c'est dans un dossier \u00e0 son nom que se trouveront tous les crawls et leur r\u00e9sultats \n\n la base de donn\u00e9es en back-end qui g\u00e8re les param\u00eatres des crawls et leur r\u00e9currence\n\n l'environnement dans lequel seront stock\u00e9s tous les dossiers de tous les utilisateurs\n\n l'url o\u00f9 sont expos\u00e9s l'avancement du crawl et ses param\u00eatres(TO DO)\n\n\nLe fichier disponible \nici\n se pr\u00e9sente sous cette forme\n\n\n{\n    \nuser\n:{\n        \nusername\n: \nuser@cortext.net\n,\n        \npassword\n: \nkeepitsecret\n\n        },\n    \ndb\n: {\n        \nprovider\n: \nmongo\n,\n        \nhost\n: \nlocalhost\n,\n        \nport\n: 27017,\n        \npassword\n: \n,\n        \ndb_name\n: \ndemo_crawtext\n,\n        \ncollection\n: \nprojects\n\n    },\n    \nenv\n: {\n        \ndirectory\n: \n,\n        \nname\n: \ncrawtext\n\n        },\n    \nwebsite\n:{\n        \nhost\n: \nlocalhost\n,\n        \nport\n: 8080\n    }\n}\n\n\n\n\nIl suffit d'en modifier les valeurs et Crawtext met \u00e0 jour la configuration et le param\u00eatrage \u00e0 chaque lancement d'un crawl.\nIl est recommand\u00e9 pour les d\u00e9butants de ne changer que le \nusername\n\nVoir le \nfichier de configuration\n\n\nParam\u00e9trage d'un projet\n\n\nLa cr\u00e9ation ou mise \u00e0 jour d'un projet se fait via un fichier au format json de parametrages du projet\nUn example de parametrage est donn\u00e9 dans \nconfig/example.json\n\nd\u00e9finir son projet. \nLe d\u00e9tail des valeurs et leur fonctions est expliqu\u00e9 dans le tutoriel et plus en d\u00e9tail dans l'API et le glossaire\n\n\n{\n    #definir le nom du projet de crawl\n    \nname\n: \nCOP21\n,\n    #activer les filtres en mettant \nactive\n:true \n    \nfilters\n: {\n        #profondeur maximale\n        \ndepth\n:{ \n            \nactive\n: true,\n            \ndepth\n: 5\n            },\n        #filtre de langue\n        \nlang\n:{\n            \nactive\n: false,\n\n            \nlang\n: \nen\n\n            },\n        # expression de recherche\n        \nquery\n:{\n            \nactive\n: false,\n            \nquery\n: \n(COP 21) OR (COP21)\n\n            }\n    },\n    #fr\u00e9quence du crawl\n    \nscheduler\n: {\n        \nactive\n: true,\n        \ndays\n: 7\n    },\n    #point de d\u00e9part du crawl (seeds)\n    \nseeds\n: {\n        \nurl\n:{\n            \nactive\n: false,\n            \nurl\n: \nhttp://www.lefigaro.fr\n\n            },\n        \nfile\n:{\n            \nactive\n: false,\n            \nfile\n: \n./config/sources.txt\n\n            },\n        \nsearch\n: {\n            \nactive\n: true,\n            \nkey\n: \nAPIKeyGivenByBing\n,\n            \nnb\n: 100\n            }\n        }\n}", 
            "title": "Configuration"
        }, 
        {
            "location": "/configuration/#configuration", 
            "text": "Crawtext se configure \u00e0 deux niveaux:  Environnement  Projet", 
            "title": "Configuration"
        }, 
        {
            "location": "/configuration/#environnement-par-defaut", 
            "text": "Crawtext propose un fichier de configuration par d\u00e9faut:  config/settings.json  Ce fichier d\u00e9finit l'environnement de fonctionnement du crawler:  l'utilisateur du crawler: c'est dans un dossier \u00e0 son nom que se trouveront tous les crawls et leur r\u00e9sultats   la base de donn\u00e9es en back-end qui g\u00e8re les param\u00eatres des crawls et leur r\u00e9currence  l'environnement dans lequel seront stock\u00e9s tous les dossiers de tous les utilisateurs  l'url o\u00f9 sont expos\u00e9s l'avancement du crawl et ses param\u00eatres(TO DO)  Le fichier disponible  ici  se pr\u00e9sente sous cette forme  {\n     user :{\n         username :  user@cortext.net ,\n         password :  keepitsecret \n        },\n     db : {\n         provider :  mongo ,\n         host :  localhost ,\n         port : 27017,\n         password :  ,\n         db_name :  demo_crawtext ,\n         collection :  projects \n    },\n     env : {\n         directory :  ,\n         name :  crawtext \n        },\n     website :{\n         host :  localhost ,\n         port : 8080\n    }\n}  Il suffit d'en modifier les valeurs et Crawtext met \u00e0 jour la configuration et le param\u00eatrage \u00e0 chaque lancement d'un crawl.\nIl est recommand\u00e9 pour les d\u00e9butants de ne changer que le  username \nVoir le  fichier de configuration", 
            "title": "Environnement par d\u00e9faut"
        }, 
        {
            "location": "/configuration/#parametrage-dun-projet", 
            "text": "La cr\u00e9ation ou mise \u00e0 jour d'un projet se fait via un fichier au format json de parametrages du projet\nUn example de parametrage est donn\u00e9 dans  config/example.json \nd\u00e9finir son projet. \nLe d\u00e9tail des valeurs et leur fonctions est expliqu\u00e9 dans le tutoriel et plus en d\u00e9tail dans l'API et le glossaire  {\n    #definir le nom du projet de crawl\n     name :  COP21 ,\n    #activer les filtres en mettant  active :true \n     filters : {\n        #profondeur maximale\n         depth :{ \n             active : true,\n             depth : 5\n            },\n        #filtre de langue\n         lang :{\n             active : false,\n\n             lang :  en \n            },\n        # expression de recherche\n         query :{\n             active : false,\n             query :  (COP 21) OR (COP21) \n            }\n    },\n    #fr\u00e9quence du crawl\n     scheduler : {\n         active : true,\n         days : 7\n    },\n    #point de d\u00e9part du crawl (seeds)\n     seeds : {\n         url :{\n             active : false,\n             url :  http://www.lefigaro.fr \n            },\n         file :{\n             active : false,\n             file :  ./config/sources.txt \n            },\n         search : {\n             active : true,\n             key :  APIKeyGivenByBing ,\n             nb : 100\n            }\n        }\n}", 
            "title": "Param\u00e9trage d'un projet"
        }, 
        {
            "location": "/tutorial/", 
            "text": "Tutoriel\n\n\nNotre premier crawl cibl\u00e9\n\n\nIci nous allons cr\u00e9er pour l'exmple un crawl cibl\u00e9 autour \ndes prises de paroles en ligne sur la loi travail\n\n\nPour cela  nous allons cr\u00e9er le projet loi_travail\n  \nname: \"loi_travail\"\n\n\nPour un crawl cibl\u00e9 autour d'une th\u00e9matique nous avons besoin\nd'une expression de recherche que nous devons d\u00e9finir avec soin\nelle ne doit \u00eatre ni trop large ni trop \u00e9troite pour permettre au crawler \nde collecter les informations sur la pol\u00e9miques\n\n\nNous allons modifier dans la fichier la \"query\":\n  ```\n  \"query\":{\n\n\n        \"active\": true, \n        \"query\": \"loi AND (Travail OR El K?omri)\" \n        }\n\n\n\n```\nPlus de d\u00e9tails sur les expressions de recherche et leur syntaxe: Glossaire\n\n\nAu vu du bruit m\u00e9diatique autour de ce sujet nous allons limiter\nla  profondeur du crawl \u00e0 3 soit le r\u00e9sultats des recherches + 2 niveaux\npour ne pas surcharger le crawler\n\n\n```\n depth:{\n         \"active\": true, \n         \"depth\":3\n         },     \n\n\nPlus de d\u00e9tails sur la profondeur d'uncrawl: Glossaire\n\nLe contenu qui nous int\u00e9resse est en fran\u00e7ais et on d\u00e9sire filtrer la langue cible\n ```\n \nlang\n:{\n         \nactive\n: true, \n         \nlang\n:\nfr\n,\n         },     \n\n\n\n\nPlus de d\u00e9tails sur les langues support\u00e9es: Glossaire\n\n\nLe crawl doit partir d'une recherche initiale sur BING\nnous allons modifier la partie seeds\n\n ajouter la cl\u00e9 d'API pour activer la recherche en ligne\n\n mettre le nombre de r\u00e9sultats que nous voulons r\u00e9cup\u00e9rer (50)\n* et d\u00e9sactiver les autres options en mettat \u00e0 false\n\n\nPlus de d\u00e9tails sur le fonctionnement de recherche sur Bing: Glossaire\n\n\nseeds\n: {\n        \nurl\n:{\n            \nactive\n: false,\n            \nurl\n: \n\n            },\n        \nfile\n:{\n            \nactive\n: false,\n            \nfile\n: \n\n            },\n        \nsearch\n: {\n            \nactive\n: true,\n            \nkey\n: \nJ8zre1019v/dIT0oXXXXXXXXX\n,\n            \nnb\n: 50\n            }\n        }```\n\nLe crawl sera r\u00e9p\u00e9t\u00e9 toutes les semaines (exprim\u00e9es en jour)\n\nscheduler\n: {\n        \nactive\n: true,\n        \ndays\n: 7\n    },\n\nPlus de d\u00e9tails sur les routines: Glossaire\n\nLa configuration de otre crawl prend donc cette forme:\n\n\n\n\n{\n    \"name\": \"loi_travail\",\n    \"filters\": {\n        \"depth\":{ \n            \"active\": true,\n            \"depth\": 3\n            },\n        \"lang\":{\n            \"active\": true,\n            \"lang\": \"fr\"\n            },\n        \"query\":{\n            \"active\": true,\n            \"query\": \"Loi AND (travail OR el khomri)\"\n            }\n    },\n    \"scheduler\": {\n        \"active\": true,\n        \"days\": 30\n    },\n    \"seeds\": {\n        \"url\":{\n            \"active\": false,\n            \"url\": \"\"\n            },\n        \"file\":{\n            \"active\": false,\n            \"file\": \"\"\n            },\n        \"search\": {\n            \"active\": true,\n            \"key\": \"MyApiSECRETKeydIT0o\",\n            \"nb\": 50\n            }\n        }\n}\n\n\n Enregistrons le au m\u00eame endroit dans un fichier final appel\u00e9 loi_travail.json \nNous allons maintenant charger la configuration \n\nOuvrons le terminal activons le virtual env et d\u00e9placons nous jusqu'aux fichiers sources de crawtext\n\n\n\n\n(venv) me@ordi:$~ cd crawtext\n(venv) me@ordi:$~/crawtext/ python crawtext setup --project=loi_travail\n\n\nIl suffit maintenant de le lancer en utilisant la comande start\n\n\n\n\n(venv) me@ordi:$~ \n(venv) me@ordi:$~/crawtext/ python crawtext start --project=loi_travail\n```\n Le crawl a commenc\u00e9 et durera tant qu'il aura des pages pertinentes \u00e0 traiter", 
            "title": "Tutorial"
        }, 
        {
            "location": "/tutorial/#tutoriel", 
            "text": "", 
            "title": "Tutoriel"
        }, 
        {
            "location": "/tutorial/#notre-premier-crawl-cible", 
            "text": "Ici nous allons cr\u00e9er pour l'exmple un crawl cibl\u00e9 autour \ndes prises de paroles en ligne sur la loi travail  Pour cela  nous allons cr\u00e9er le projet loi_travail\n   name: \"loi_travail\"  Pour un crawl cibl\u00e9 autour d'une th\u00e9matique nous avons besoin\nd'une expression de recherche que nous devons d\u00e9finir avec soin\nelle ne doit \u00eatre ni trop large ni trop \u00e9troite pour permettre au crawler \nde collecter les informations sur la pol\u00e9miques  Nous allons modifier dans la fichier la \"query\":\n  ```\n  \"query\":{          \"active\": true, \n        \"query\": \"loi AND (Travail OR El K?omri)\" \n        }  ```\nPlus de d\u00e9tails sur les expressions de recherche et leur syntaxe: Glossaire  Au vu du bruit m\u00e9diatique autour de ce sujet nous allons limiter\nla  profondeur du crawl \u00e0 3 soit le r\u00e9sultats des recherches + 2 niveaux\npour ne pas surcharger le crawler  ```\n depth:{\n         \"active\": true, \n         \"depth\":3\n         },       Plus de d\u00e9tails sur la profondeur d'uncrawl: Glossaire\n\nLe contenu qui nous int\u00e9resse est en fran\u00e7ais et on d\u00e9sire filtrer la langue cible\n ```\n  lang :{\n          active : true, \n          lang : fr ,\n         },       Plus de d\u00e9tails sur les langues support\u00e9es: Glossaire  Le crawl doit partir d'une recherche initiale sur BING\nnous allons modifier la partie seeds  ajouter la cl\u00e9 d'API pour activer la recherche en ligne  mettre le nombre de r\u00e9sultats que nous voulons r\u00e9cup\u00e9rer (50)\n* et d\u00e9sactiver les autres options en mettat \u00e0 false  Plus de d\u00e9tails sur le fonctionnement de recherche sur Bing: Glossaire  seeds : {\n         url :{\n             active : false,\n             url :  \n            },\n         file :{\n             active : false,\n             file :  \n            },\n         search : {\n             active : true,\n             key :  J8zre1019v/dIT0oXXXXXXXXX ,\n             nb : 50\n            }\n        }```\n\nLe crawl sera r\u00e9p\u00e9t\u00e9 toutes les semaines (exprim\u00e9es en jour) scheduler : {\n         active : true,\n         days : 7\n    },\n\nPlus de d\u00e9tails sur les routines: Glossaire\n\nLa configuration de otre crawl prend donc cette forme:  {\n    \"name\": \"loi_travail\",\n    \"filters\": {\n        \"depth\":{ \n            \"active\": true,\n            \"depth\": 3\n            },\n        \"lang\":{\n            \"active\": true,\n            \"lang\": \"fr\"\n            },\n        \"query\":{\n            \"active\": true,\n            \"query\": \"Loi AND (travail OR el khomri)\"\n            }\n    },\n    \"scheduler\": {\n        \"active\": true,\n        \"days\": 30\n    },\n    \"seeds\": {\n        \"url\":{\n            \"active\": false,\n            \"url\": \"\"\n            },\n        \"file\":{\n            \"active\": false,\n            \"file\": \"\"\n            },\n        \"search\": {\n            \"active\": true,\n            \"key\": \"MyApiSECRETKeydIT0o\",\n            \"nb\": 50\n            }\n        }\n}   Enregistrons le au m\u00eame endroit dans un fichier final appel\u00e9 loi_travail.json \nNous allons maintenant charger la configuration \n\nOuvrons le terminal activons le virtual env et d\u00e9placons nous jusqu'aux fichiers sources de crawtext  (venv) me@ordi:$~ cd crawtext\n(venv) me@ordi:$~/crawtext/ python crawtext setup --project=loi_travail  Il suffit maintenant de le lancer en utilisant la comande start  (venv) me@ordi:$~ \n(venv) me@ordi:$~/crawtext/ python crawtext start --project=loi_travail\n```\n Le crawl a commenc\u00e9 et durera tant qu'il aura des pages pertinentes \u00e0 traiter", 
            "title": "Notre premier crawl cibl\u00e9"
        }, 
        {
            "location": "/developper_guide/", 
            "text": "WARNING -  The page \"introduction.md\" contained a hyperlink to \"config.md\" which is not listed in the \"pages\" configuration. \nWARNING -  The page \"introduction.md\" contained a hyperlink to \"config.md\" which is not listed in the \"pages\" configuration. \nWARNING -  The page \"introduction.md\" contained a hyperlink to \"dev.md\" which is not listed in the \"pages\" configuration. \nWARNING -  The page \"introduction.md\" contained a hyperlink to \"todo.md\" which is not listed in the \"pages\" configuration. \nWARNING -  The page \"introduction.md\" contained a hyperlink to \"changelog.md\" which is not listed in the \"pages\" configuration. \nWARNING -  The page \"installation.md\" contained a hyperlink to \"developper_guide.md\" which is not listed in the \"pages\" configuration.", 
            "title": "Developper guide"
        }, 
        {
            "location": "/todo/", 
            "text": "", 
            "title": "Next steps developpement"
        }
    ]
}